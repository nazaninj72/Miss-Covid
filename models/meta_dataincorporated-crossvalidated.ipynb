{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fcclassifier import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "Device name: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text=text.replace(\"#\",\"\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/nazaninjafar/ds4cg2020/bert-covid/data/alldata2.tsv')\n",
    "meta_data=pd.read_csv('/home/nazaninjafar/ds4cg2020/bert-covid/data/user_metadata.tsv')\n",
    "meta_data=meta_data.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "def norm(X):\n",
    "    X=normalize(X, axis=0, norm='max')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User metadata features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = np.expand_dims((meta_data.friend_count.values), axis=1)\n",
    "fwc = np.expand_dims(meta_data.follower_count.values, axis=1)\n",
    "favc = np.expand_dims(meta_data.fav_count.values, axis=1)\n",
    "tc = np.expand_dims(meta_data.tweet_count.values, axis=1)\n",
    "TFF=fc+1/fwc+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_times=meta_data.created.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-03 13:35:54.666334\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "# import datetime\n",
    "\n",
    "today = datetime.datetime.today()\n",
    "print(today)\n",
    "time_difference=[]\n",
    "for a in creation_times:\n",
    "    account_date=datetime.datetime.strptime(a, '%Y-%m-%d %H:%M:%S')\n",
    "    time_difference.append((today - account_date).days)\n",
    "#     print(time_difference)\n",
    "# age=[(today - account_date).days for ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "age=np.array(time_difference).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_X=np.concatenate((favc,tc,TFF,age),axis=1)\n",
    "md_X=norm(md_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = data.tweet.values\n",
    "# y = data.label.values\n",
    "# indices = np.arange(len(X))\n",
    "# train_idx, val_idx, y_train, y_val= train_test_split(indices, y,stratify = y, test_size=0.2, random_state=42)\n",
    "# X_train = X[train_idx]\n",
    "# X_val = X[val_idx]\n",
    "# mdX_train = md_X[train_idx]\n",
    "# mdX_val = md_X[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# X_train, X_val, y_train, y_val =\\\n",
    "#     train_test_split(X, y,stratify = y, test_size=0.1, random_state=42)\n",
    "# mdX_train, mdX_val, y_train, y_val =\\\n",
    "#     train_test_split(md_X, y,stratify = y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_ctbert(data,max_len):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max_len,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,     # Return attention mask\n",
    "            truncation=True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Tokenizing data...')\n",
    "# max_len = 160\n",
    "# train_inputs, train_masks = preprocessing_for_ctbert(X_train,max_len)\n",
    "# val_inputs, val_masks = preprocessing_for_ctbert(X_val,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningrate=5e-5\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from fcclassifier import *\n",
    "# from bertmodel import *\n",
    "def initialize_model(epochs=4,lr=learningrate):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "#     bert_model = BertM(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    classifier=MergedClassifier()\n",
    "    \n",
    "    classifier.to(device)\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(classifier.parameters(),\n",
    "                      lr=learningrate,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from fcclassifier import *\n",
    "# from bertmodel import *\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(classifier, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} |{'Train Acc':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        classifier.train()\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask,b_md, b_labels = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            # Zero out any previously calculated gradients\n",
    "#             bertmodel.zero_grad()\n",
    "            classifier.zero_grad()\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = classifier(b_input_ids, b_attn_mask,b_md)\n",
    "            \n",
    "#             #concatenating the hidden state with metadata:\n",
    "#             out=torch.cat((h_state,b_md),axis=1)\n",
    "#             print(out.size())  \n",
    "              \n",
    "#             logits = classifier(out)\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(classifier.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy,_ = evaluate(classifier, val_dataloader)\n",
    "            _,train_accuracy,_=evaluate(classifier,train_dataloader)\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {train_accuracy:^9.2f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(classifier, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    classifier.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    preds=[]\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask,b_md, b_labels = tuple(t.to(device) for t in batch)\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = classifier(b_input_ids, b_attn_mask,b_md)\n",
    "            #concatenating the hidden state with metadata:\n",
    "#             out=torch.cat((h_state,b_md),axis=1)\n",
    "            \n",
    "#             logits = classifier(out)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        pred= torch.argmax(logits, dim=1).flatten()\n",
    "        preds.append(pred)\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (pred == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy,preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "def cval_eval(val_loss,val_accuracy,preds,y_val):\n",
    "        metrics={}\n",
    "        np_preds=[]\n",
    "        for i in preds:\n",
    "            b=i.cpu().detach().numpy()\n",
    "            np_preds=np.append(np_preds,b,axis=0)\n",
    "        metrics['val_accuracy']=val_accuracy\n",
    "        metrics['f1']=f1_score(y_val, np_preds, average=None)\n",
    "        metrics['precision']=precision_score(y_val, np_preds, average=None)\n",
    "    #     metrics['precision']=average_precision_score(y_val, probs)\n",
    "        metrics['recall']=recall_score(y_val, np_preds, average=None)\n",
    "        print(metrics)\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask,b_md,b_label = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask,b_md)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "#     # Get accuracy over the test set\n",
    "#     y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "#     accuracy = accuracy_score(y_true, y_pred)\n",
    "#     print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "#     # Plot ROC AUC\n",
    "#     plt.title('Receiver Operating Characteristic')\n",
    "#     plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "#     plt.legend(loc = 'lower right')\n",
    "#     plt.plot([0, 1], [0, 1],'r--')\n",
    "#     plt.xlim([0, 1])\n",
    "#     plt.ylim([0, 1])\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1\n",
      " 0 0 1 1 0 1 0 0]\n",
      "(45,)\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   12    |   0.688314   |     -      |     -     |   4.17   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.688314   |   57.56   |  0.675513  |   55.05   |   5.60   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   12    |   0.640616   |     -      |     -     |   4.06   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.640616   |   86.51   |  0.659778  |   53.49   |   5.50   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   12    |   0.545792   |     -      |     -     |   4.13   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.545792   |   90.32   |  0.653621  |   55.05   |   5.57   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   12    |   0.441354   |     -      |     -     |   4.13   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.441354   |   97.12   |  0.644166  |   63.58   |   5.58   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   12    |   0.350619   |     -      |     -     |   4.18   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.350619   |   97.36   |  0.683006  |   65.87   |   5.63   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |   12    |   0.319671   |     -      |     -     |   4.15   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.319671   |   97.84   |  0.676142  |   67.43   |   5.60   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   7    |   12    |   0.299865   |     -      |     -     |   4.16   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.299865   |   97.56   |  0.632439  |   71.27   |   5.61   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |   12    |   0.293745   |     -      |     -     |   4.17   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.293745   |   97.80   |  0.828871  |   62.02   |   5.63   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   9    |   12    |   0.279571   |     -      |     -     |   4.15   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.279571   |   98.80   |  1.013271  |   63.58   |   5.61   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  10    |   12    |   0.265972   |     -      |     -     |   4.17   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.265972   |   99.28   |  0.797556  |   64.30   |   5.63   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  11    |   12    |   0.261015   |     -      |     -     |   4.16   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.261015   |   99.28   |  0.874082  |   64.30   |   5.63   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  12    |   12    |   0.263047   |     -      |     -     |   4.19   \n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   0.263047   |   99.28   |  0.757729  |   68.15   |   5.66   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  13    |   12    |   0.258544   |     -      |     -     |   4.17   \n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   0.258544   |   99.28   |  0.870274  |   64.30   |   5.64   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  14    |   12    |   0.252341   |     -      |     -     |   4.17   \n",
      "----------------------------------------------------------------------\n",
      "  14    |    -    |   0.252341   |   99.01   |  0.823351  |   66.59   |   5.64   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  15    |   12    |   0.251073   |     -      |     -     |   4.16   \n",
      "----------------------------------------------------------------------\n",
      "  15    |    -    |   0.251073   |   99.28   |  0.831460  |   66.59   |   5.63   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  16    |   12    |   0.252668   |     -      |     -     |   4.16   \n",
      "----------------------------------------------------------------------\n",
      "  16    |    -    |   0.252668   |   99.28   |  0.986893  |   57.33   |   5.63   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  17    |   12    |   0.251505   |     -      |     -     |   4.17   \n",
      "----------------------------------------------------------------------\n",
      "  17    |    -    |   0.251505   |   99.28   |  0.925857  |   66.59   |   5.65   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  18    |   12    |   0.250808   |     -      |     -     |   4.14   \n",
      "----------------------------------------------------------------------\n",
      "  18    |    -    |   0.250808   |   99.28   |  0.859964  |   66.59   |   5.61   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  19    |   12    |   0.246136   |     -      |     -     |   4.07   \n",
      "----------------------------------------------------------------------\n",
      "  19    |    -    |   0.246136   |   99.28   |  0.868916  |   66.59   |   5.54   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  20    |   12    |   0.251361   |     -      |     -     |   4.16   \n",
      "----------------------------------------------------------------------\n",
      "  20    |    -    |   0.251361   |   99.28   |  0.874644  |   66.59   |   5.64   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "{'val_accuracy': 66.58653846153847, 'f1': array([0.56410256, 0.66666667]), 'precision': array([0.57894737, 0.65384615]), 'recall': array([0.55, 0.68])}\n",
      "66.58653846153847\n",
      "[1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1\n",
      " 0 0 1 0 0 0 0 0]\n",
      "(45,)\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   12    |   0.715510   |     -      |     -     |   4.21   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.715510   |   55.82   |  0.698750  |   44.35   |   5.67   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   12    |   0.682328   |     -      |     -     |   4.18   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.682328   |   60.56   |  0.694589  |   45.91   |   5.66   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   12    |   0.615914   |     -      |     -     |   4.22   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.615914   |   87.37   |  0.627868  |   65.14   |   5.69   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   12    |   0.523055   |     -      |     -     |   4.22   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.523055   |   94.17   |  0.595589  |   72.84   |   5.69   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   12    |   0.447038   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.447038   |   96.39   |  0.617953  |   69.71   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |   12    |   0.406024   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.406024   |   95.91   |  0.618138  |   67.43   |   5.70   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   7    |   12    |   0.381928   |     -      |     -     |   4.19   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.381928   |   94.23   |  0.616240  |   69.83   |   5.66   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |   12    |   0.372369   |     -      |     -     |   4.19   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.372369   |   97.60   |  0.652519  |   67.43   |   5.66   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   9    |   12    |   0.345712   |     -      |     -     |   4.18   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.345712   |   98.32   |  0.649857  |   62.74   |   5.65   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  10    |   12    |   0.336824   |     -      |     -     |   4.21   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.336824   |   98.32   |  0.682157  |   58.89   |   5.68   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  11    |   12    |   0.315137   |     -      |     -     |   4.18   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.315137   |   98.32   |  0.694171  |   62.02   |   5.66   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  12    |   12    |   0.300923   |     -      |     -     |   4.20   \n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   0.300923   |   98.32   |  0.702593  |   62.02   |   5.67   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  13    |   12    |   0.292275   |     -      |     -     |   4.19   \n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   0.292275   |   98.32   |  0.692141  |   68.27   |   5.66   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  14    |   12    |   0.289626   |     -      |     -     |   4.18   \n",
      "----------------------------------------------------------------------\n",
      "  14    |    -    |   0.289626   |   98.04   |  0.705243  |   66.71   |   5.66   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  15    |   12    |   0.280558   |     -      |     -     |   4.19   \n",
      "----------------------------------------------------------------------\n",
      "  15    |    -    |   0.280558   |   98.32   |  0.714319  |   65.14   |   5.67   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  16    |   12    |   0.275851   |     -      |     -     |   4.17   \n",
      "----------------------------------------------------------------------\n",
      "  16    |    -    |   0.275851   |   98.29   |  0.719023  |   65.14   |   5.64   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  17    |   12    |   0.274918   |     -      |     -     |   4.22   \n",
      "----------------------------------------------------------------------\n",
      "  17    |    -    |   0.274918   |   98.29   |  0.706345  |   65.14   |   5.69   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  18    |   12    |   0.269099   |     -      |     -     |   4.22   \n",
      "----------------------------------------------------------------------\n",
      "  18    |    -    |   0.269099   |   98.56   |  0.709252  |   65.14   |   5.70   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  19    |   12    |   0.269287   |     -      |     -     |   4.21   \n",
      "----------------------------------------------------------------------\n",
      "  19    |    -    |   0.269287   |   98.56   |  0.711799  |   65.14   |   5.69   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  20    |   12    |   0.264842   |     -      |     -     |   4.21   \n",
      "----------------------------------------------------------------------\n",
      "  20    |    -    |   0.264842   |   98.29   |  0.713039  |   65.14   |   5.69   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "{'val_accuracy': 65.14423076923077, 'f1': array([0.66666667, 0.66666667]), 'precision': array([0.6 , 0.75]), 'recall': array([0.75, 0.6 ])}\n",
      "65.14423076923077\n",
      "[1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "(45,)\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   12    |   0.716964   |     -      |     -     |   4.28   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.716964   |   56.27   |  0.693793  |   49.76   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   12    |   0.680077   |     -      |     -     |   4.18   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.680077   |   61.55   |  0.680113  |   51.32   |   5.65   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   12    |   0.620658   |     -      |     -     |   4.20   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.620658   |   87.64   |  0.626876  |   69.83   |   5.68   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   12    |   0.536782   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.536782   |   84.01   |  0.604538  |   68.87   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   12    |   0.484162   |     -      |     -     |   4.20   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.484162   |   96.63   |  0.553049  |   77.52   |   5.68   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |   12    |   0.417898   |     -      |     -     |   4.33   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.417898   |   95.43   |  0.611186  |   71.39   |   5.81   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   7    |   12    |   0.390456   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.390456   |   98.08   |  0.561399  |   75.24   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |   12    |   0.358765   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.358765   |   98.08   |  0.547060  |   82.21   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   9    |   12    |   0.336967   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.336967   |   98.56   |  0.569748  |   76.80   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  10    |   12    |   0.325621   |     -      |     -     |   4.21   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.325621   |   98.56   |  0.521658  |   80.65   |   5.69   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  11    |   12    |   0.310296   |     -      |     -     |   4.34   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.310296   |   98.80   |  0.529357  |   79.93   |   5.81   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  12    |   12    |   0.291697   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   0.291697   |   99.04   |  0.570782  |   78.37   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  13    |   12    |   0.283864   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   0.283864   |   99.04   |  0.469924  |   84.50   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  14    |   12    |   0.279731   |     -      |     -     |   4.22   \n",
      "----------------------------------------------------------------------\n",
      "  14    |    -    |   0.279731   |   98.77   |  0.466302  |   84.50   |   5.70   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  15    |   12    |   0.272495   |     -      |     -     |   4.21   \n",
      "----------------------------------------------------------------------\n",
      "  15    |    -    |   0.272495   |   99.04   |  0.489837  |   83.77   |   5.68   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  16    |   12    |   0.265933   |     -      |     -     |   4.32   \n",
      "----------------------------------------------------------------------\n",
      "  16    |    -    |   0.265933   |   98.77   |  0.491447  |   83.77   |   5.80   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  17    |   12    |   0.262078   |     -      |     -     |   4.30   \n",
      "----------------------------------------------------------------------\n",
      "  17    |    -    |   0.262078   |   99.04   |  0.491921  |   83.77   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  18    |   12    |   0.259406   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "  18    |    -    |   0.259406   |   99.04   |  0.492354  |   83.77   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  19    |   12    |   0.259002   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "  19    |    -    |   0.259002   |   99.04   |  0.492567  |   83.77   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  20    |   12    |   0.254861   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "  20    |    -    |   0.254861   |   99.04   |  0.492658  |   83.77   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "{'val_accuracy': 83.77403846153847, 'f1': array([0.85, 0.88]), 'precision': array([0.85, 0.88]), 'recall': array([0.85, 0.88])}\n",
      "83.77403846153847\n",
      "[1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1\n",
      " 0 0 1 1 1 0 1 0]\n",
      "(45,)\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   12    |   0.715797   |     -      |     -     |   4.18   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.715797   |   56.27   |  0.689913  |   52.76   |   5.65   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   12    |   0.679418   |     -      |     -     |   4.20   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.679418   |   60.74   |  0.674425  |   55.89   |   5.67   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   12    |   0.619032   |     -      |     -     |   4.24   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.619032   |   83.29   |  0.647597  |   65.14   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   12    |   0.532315   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.532315   |   93.48   |  0.627684  |   61.30   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   12    |   0.451597   |     -      |     -     |   4.24   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.451597   |   94.95   |  0.662575  |   62.86   |   5.72   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |   12    |   0.405292   |     -      |     -     |   4.24   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.405292   |   97.12   |  0.640838  |   66.71   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   7    |   12    |   0.384945   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.384945   |   98.56   |  0.600860  |   72.96   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |   12    |   0.355449   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.355449   |   98.80   |  0.611615  |   71.39   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   9    |   12    |   0.331809   |     -      |     -     |   4.32   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.331809   |   98.80   |  0.634780  |   69.83   |   5.80   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  10    |   12    |   0.321661   |     -      |     -     |   4.24   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.321661   |   98.80   |  0.639638  |   69.83   |   5.72   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  11    |   12    |   0.305603   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.305603   |   98.80   |  0.641723  |   71.39   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  12    |   12    |   0.293883   |     -      |     -     |   4.33   \n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   0.293883   |   98.80   |  0.646078  |   71.39   |   5.81   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  13    |   12    |   0.285773   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   0.285773   |   98.80   |  0.676291  |   69.83   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  14    |   12    |   0.279758   |     -      |     -     |   4.40   \n",
      "----------------------------------------------------------------------\n",
      "  14    |    -    |   0.279758   |   98.53   |  0.670699  |   69.83   |   5.89   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  15    |   12    |   0.274821   |     -      |     -     |   4.31   \n",
      "----------------------------------------------------------------------\n",
      "  15    |    -    |   0.274821   |   98.80   |  0.678384  |   69.83   |   5.79   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  16    |   12    |   0.270427   |     -      |     -     |   4.32   \n",
      "----------------------------------------------------------------------\n",
      "  16    |    -    |   0.270427   |   98.53   |  0.690573  |   68.27   |   5.80   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  17    |   12    |   0.266246   |     -      |     -     |   4.33   \n",
      "----------------------------------------------------------------------\n",
      "  17    |    -    |   0.266246   |   98.53   |  0.685603  |   71.39   |   5.81   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  18    |   12    |   0.264316   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "  18    |    -    |   0.264316   |   98.80   |  0.681041  |   71.39   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  19    |   12    |   0.263992   |     -      |     -     |   4.33   \n",
      "----------------------------------------------------------------------\n",
      "  19    |    -    |   0.263992   |   98.80   |  0.683279  |   71.39   |   5.81   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  20    |   12    |   0.257674   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "  20    |    -    |   0.257674   |   98.53   |  0.684300  |   71.39   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "{'val_accuracy': 71.39423076923077, 'f1': array([0.68571429, 0.8       ]), 'precision': array([0.8       , 0.73333333]), 'recall': array([0.6 , 0.88])}\n",
      "71.39423076923077\n",
      "[1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1\n",
      " 0 1 0 0 1 0 1 1]\n",
      "(45,)\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   12    |   0.714947   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.714947   |   55.54   |  0.685917  |   55.05   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   12    |   0.680062   |     -      |     -     |   4.25   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.680062   |   63.00   |  0.667485  |   58.17   |   5.72   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   12    |   0.615895   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.615895   |   81.15   |  0.655938  |   62.86   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   12    |   0.523156   |     -      |     -     |   4.24   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.523156   |   93.96   |  0.584753  |   75.24   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   12    |   0.458588   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.458588   |   95.91   |  0.559238  |   73.68   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |   12    |   0.413259   |     -      |     -     |   4.30   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.413259   |   97.12   |  0.472045  |   87.62   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   7    |   12    |   0.383902   |     -      |     -     |   4.25   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.383902   |   95.91   |  0.507851  |   82.93   |   5.73   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |   12    |   0.363242   |     -      |     -     |   4.21   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.363242   |   98.08   |  0.531094  |   80.65   |   5.68   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   9    |   12    |   0.340989   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.340989   |   98.32   |  0.516837  |   78.25   |   5.72   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  10    |   12    |   0.327170   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.327170   |   98.32   |  0.496568  |   81.37   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  11    |   12    |   0.313173   |     -      |     -     |   4.25   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.313173   |   98.32   |  0.514536  |   77.52   |   5.72   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  12    |   12    |   0.306389   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   0.306389   |   98.32   |  0.526173  |   79.09   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  13    |   12    |   0.292103   |     -      |     -     |   4.28   \n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   0.292103   |   98.56   |  0.519551  |   79.09   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  14    |   12    |   0.284935   |     -      |     -     |   4.25   \n",
      "----------------------------------------------------------------------\n",
      "  14    |    -    |   0.284935   |   98.56   |  0.488881  |   78.25   |   5.73   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  15    |   12    |   0.283725   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "  15    |    -    |   0.283725   |   99.04   |  0.541374  |   79.09   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  16    |   12    |   0.280934   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "  16    |    -    |   0.280934   |   98.77   |  0.454269  |   83.65   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  17    |   12    |   0.269172   |     -      |     -     |   4.30   \n",
      "----------------------------------------------------------------------\n",
      "  17    |    -    |   0.269172   |   99.04   |  0.439251  |   85.22   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  18    |   12    |   0.264906   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "  18    |    -    |   0.264906   |   99.04   |  0.470362  |   82.93   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  19    |   12    |   0.264435   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "  19    |    -    |   0.264435   |   99.04   |  0.473482  |   84.50   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  20    |   12    |   0.259434   |     -      |     -     |   4.25   \n",
      "----------------------------------------------------------------------\n",
      "  20    |    -    |   0.259434   |   99.04   |  0.473492  |   84.50   |   5.73   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "{'val_accuracy': 84.4951923076923, 'f1': array([0.8       , 0.87272727]), 'precision': array([0.93333333, 0.8       ]), 'recall': array([0.7 , 0.96])}\n",
      "84.4951923076923\n",
      "[0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1]\n",
      "(45,)\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   12    |   0.712633   |     -      |     -     |   4.22   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.712633   |   55.54   |  0.673842  |   59.62   |   5.69   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   12    |   0.669728   |     -      |     -     |   4.21   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.669728   |   83.83   |  0.640920  |   74.40   |   5.69   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   12    |   0.585205   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.585205   |   81.39   |  0.619216  |   69.71   |   5.72   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   12    |   0.507342   |     -      |     -     |   4.33   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.507342   |   97.12   |  0.539451  |   78.25   |   5.81   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   12    |   0.426902   |     -      |     -     |   4.24   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.426902   |   96.36   |  0.608369  |   72.84   |   5.72   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |   12    |   0.395224   |     -      |     -     |   4.28   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.395224   |   98.32   |  0.615427  |   71.27   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   7    |   12    |   0.368684   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.368684   |   98.56   |  0.612816  |   69.71   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |   12    |   0.346199   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.346199   |   98.80   |  0.580261  |   71.27   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9    |   12    |   0.334356   |     -      |     -     |   4.28   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.334356   |   99.04   |  0.656279  |   68.99   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  10    |   12    |   0.314191   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.314191   |   99.04   |  0.502269  |   76.68   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  11    |   12    |   0.299962   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.299962   |   99.04   |  0.627381  |   72.00   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  12    |   12    |   0.292163   |     -      |     -     |   4.28   \n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   0.292163   |   99.04   |  0.521965  |   76.68   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  13    |   12    |   0.281032   |     -      |     -     |   4.25   \n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   0.281032   |   99.28   |  0.665703  |   68.15   |   5.73   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  14    |   12    |   0.277034   |     -      |     -     |   4.28   \n",
      "----------------------------------------------------------------------\n",
      "  14    |    -    |   0.277034   |   99.28   |  0.584927  |   73.56   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  15    |   12    |   0.270736   |     -      |     -     |   4.28   \n",
      "----------------------------------------------------------------------\n",
      "  15    |    -    |   0.270736   |   99.28   |  0.539187  |   73.56   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  16    |   12    |   0.265047   |     -      |     -     |   4.24   \n",
      "----------------------------------------------------------------------\n",
      "  16    |    -    |   0.265047   |   99.01   |  0.710704  |   67.43   |   5.72   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  17    |   12    |   0.260628   |     -      |     -     |   4.31   \n",
      "----------------------------------------------------------------------\n",
      "  17    |    -    |   0.260628   |   99.01   |  0.721682  |   67.43   |   5.79   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  18    |   12    |   0.258073   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "  18    |    -    |   0.258073   |   99.28   |  0.688863  |   65.87   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  19    |   12    |   0.257769   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "  19    |    -    |   0.257769   |   99.28   |  0.675079  |   69.71   |   5.72   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  20    |   12    |   0.253145   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "  20    |    -    |   0.253145   |   99.01   |  0.671688  |   69.71   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "{'val_accuracy': 69.71153846153847, 'f1': array([0.51612903, 0.74576271]), 'precision': array([0.72727273, 0.64705882]), 'recall': array([0.4 , 0.88])}\n",
      "69.71153846153847\n",
      "[0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 0 0 0]\n",
      "(45,)\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   12    |   0.713709   |     -      |     -     |   4.28   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.713709   |   55.27   |  0.676239  |   59.62   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   12    |   0.668672   |     -      |     -     |   4.21   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.668672   |   72.92   |  0.666162  |   57.33   |   5.69   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   12    |   0.597035   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.597035   |   86.75   |  0.648807  |   60.46   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   12    |   0.502167   |     -      |     -     |   4.24   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.502167   |   96.63   |  0.592819  |   73.68   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   12    |   0.438588   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.438588   |   98.08   |  0.614880  |   66.71   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |   12    |   0.394335   |     -      |     -     |   4.25   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.394335   |   98.32   |  0.566242  |   70.55   |   5.73   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7    |   12    |   0.363672   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.363672   |   98.80   |  0.552278  |   72.12   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |   12    |   0.342623   |     -      |     -     |   4.28   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.342623   |   99.28   |  0.571368  |   70.55   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   9    |   12    |   0.338173   |     -      |     -     |   4.30   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.338173   |   97.80   |  0.704813  |   62.74   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  10    |   12    |   0.320569   |     -      |     -     |   4.25   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.320569   |   99.28   |  0.588245  |   70.55   |   5.72   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  11    |   12    |   0.304592   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.304592   |   99.28   |  0.577131  |   70.55   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  12    |   12    |   0.287733   |     -      |     -     |   4.25   \n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   0.287733   |   99.28   |  0.636014  |   67.43   |   5.73   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  13    |   12    |   0.280147   |     -      |     -     |   4.20   \n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   0.280147   |   99.28   |  0.618006  |   72.84   |   5.67   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  14    |   12    |   0.270381   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "  14    |    -    |   0.270381   |   99.28   |  0.634045  |   67.43   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  15    |   12    |   0.263884   |     -      |     -     |   4.25   \n",
      "----------------------------------------------------------------------\n",
      "  15    |    -    |   0.263884   |   99.28   |  0.637601  |   67.43   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  16    |   12    |   0.260410   |     -      |     -     |   4.28   \n",
      "----------------------------------------------------------------------\n",
      "  16    |    -    |   0.260410   |   99.01   |  0.639489  |   67.43   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  17    |   12    |   0.256692   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "  17    |    -    |   0.256692   |   99.28   |  0.640750  |   67.43   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  18    |   12    |   0.253364   |     -      |     -     |   4.28   \n",
      "----------------------------------------------------------------------\n",
      "  18    |    -    |   0.253364   |   99.28   |  0.641842  |   67.43   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  19    |   12    |   0.253744   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "  19    |    -    |   0.253744   |   99.28   |  0.642310  |   67.43   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  20    |   12    |   0.248864   |     -      |     -     |   4.31   \n",
      "----------------------------------------------------------------------\n",
      "  20    |    -    |   0.248864   |   99.28   |  0.642529  |   67.43   |   5.79   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "{'val_accuracy': 67.42788461538461, 'f1': array([0.59459459, 0.71698113]), 'precision': array([0.64705882, 0.67857143]), 'recall': array([0.55, 0.76])}\n",
      "67.42788461538461\n",
      "[1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1\n",
      " 0 1 0 1 1 1]\n",
      "(43,)\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   12    |   0.714677   |     -      |     -     |   4.39   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.714677   |   55.27   |  0.672969  |   61.36   |   5.86   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   12    |   0.670500   |     -      |     -     |   4.34   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.670500   |   80.61   |  0.638614  |   75.28   |   5.82   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   12    |   0.608372   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.608372   |   91.61   |  0.589267  |   75.28   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   12    |   0.496474   |     -      |     -     |   4.39   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.496474   |   98.08   |  0.585787  |   72.30   |   5.86   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5    |   12    |   0.418977   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.418977   |   98.80   |  0.548934  |   73.86   |   5.70   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |   12    |   0.379468   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.379468   |   99.04   |  0.496781  |   81.53   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   7    |   12    |   0.355726   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.355726   |   99.04   |  0.486242  |   86.08   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |   12    |   0.336685   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.336685   |   99.28   |  0.473273  |   82.95   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   9    |   12    |   0.320236   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.320236   |   99.28   |  0.483577  |   81.53   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  10    |   12    |   0.309607   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.309607   |   99.28   |  0.511235  |   78.41   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  11    |   12    |   0.293221   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.293221   |   99.28   |  0.496871  |   78.41   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  12    |   12    |   0.282644   |     -      |     -     |   4.30   \n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   0.282644   |   99.28   |  0.525923  |   76.85   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  13    |   12    |   0.273971   |     -      |     -     |   4.30   \n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   0.273971   |   99.07   |  0.494021  |   82.95   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  14    |   12    |   0.267503   |     -      |     -     |   4.30   \n",
      "----------------------------------------------------------------------\n",
      "  14    |    -    |   0.267503   |   99.28   |  0.550142  |   72.30   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  15    |   12    |   0.266847   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "  15    |    -    |   0.266847   |   99.07   |  0.567634  |   69.32   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  16    |   12    |   0.257637   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "  16    |    -    |   0.257637   |   99.28   |  0.530145  |   78.41   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  17    |   12    |   0.253691   |     -      |     -     |   4.31   \n",
      "----------------------------------------------------------------------\n",
      "  17    |    -    |   0.253691   |   99.28   |  0.527772  |   78.41   |   5.79   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  18    |   12    |   0.253391   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "  18    |    -    |   0.253391   |   99.28   |  0.533846  |   78.41   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  19    |   12    |   0.253780   |     -      |     -     |   4.25   \n",
      "----------------------------------------------------------------------\n",
      "  19    |    -    |   0.253780   |   99.28   |  0.537385  |   78.41   |   5.72   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  20    |   12    |   0.253609   |     -      |     -     |   4.33   \n",
      "----------------------------------------------------------------------\n",
      "  20    |    -    |   0.253609   |   99.07   |  0.538171  |   78.41   |   5.81   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "{'val_accuracy': 78.4090909090909, 'f1': array([0.73684211, 0.79166667]), 'precision': array([0.73684211, 0.79166667]), 'recall': array([0.73684211, 0.79166667])}\n",
      "78.4090909090909\n",
      "[0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0\n",
      " 1 1 1 1 1 1]\n",
      "(43,)\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   12    |   0.699854   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.699854   |   68.11   |  0.668844  |   67.61   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   12    |   0.638967   |     -      |     -     |   4.23   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.638967   |   88.04   |  0.639370  |   72.30   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3    |   12    |   0.555871   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.555871   |   92.07   |  0.557243  |   78.41   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   12    |   0.496568   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.496568   |   94.71   |  0.521192  |   82.95   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   12    |   0.433985   |     -      |     -     |   4.33   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.433985   |   96.63   |  0.466537  |   86.22   |   5.80   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |   12    |   0.399261   |     -      |     -     |   4.33   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.399261   |   96.63   |  0.539650  |   76.85   |   5.81   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   7    |   12    |   0.365126   |     -      |     -     |   4.33   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.365126   |   97.12   |  0.489367  |   84.52   |   5.80   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |   12    |   0.340801   |     -      |     -     |   4.31   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.340801   |   96.90   |  0.389999  |   89.20   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   9    |   12    |   0.322903   |     -      |     -     |   4.30   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.322903   |   97.12   |  0.455808  |   83.10   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  10    |   12    |   0.313821   |     -      |     -     |   4.30   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.313821   |   96.90   |  0.489687  |   79.97   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  11    |   12    |   0.298995   |     -      |     -     |   4.32   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.298995   |   96.88   |  0.367599  |   89.20   |   5.80   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  12    |   12    |   0.280249   |     -      |     -     |   4.31   \n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   0.280249   |   97.84   |  0.431195  |   86.08   |   5.79   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  13    |   12    |   0.270280   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   0.270280   |   97.86   |  0.424048  |   83.10   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  14    |   12    |   0.264772   |     -      |     -     |   4.34   \n",
      "----------------------------------------------------------------------\n",
      "  14    |    -    |   0.264772   |   98.08   |  0.442080  |   83.10   |   5.82   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  15    |   12    |   0.254312   |     -      |     -     |   4.31   \n",
      "----------------------------------------------------------------------\n",
      "  15    |    -    |   0.254312   |   98.11   |  0.419856  |   84.66   |   5.79   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  16    |   12    |   0.254025   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "  16    |    -    |   0.254025   |   98.32   |  0.391072  |   87.64   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  17    |   12    |   0.243393   |     -      |     -     |   4.31   \n",
      "----------------------------------------------------------------------\n",
      "  17    |    -    |   0.243393   |   98.32   |  0.394546  |   87.64   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  18    |   12    |   0.240929   |     -      |     -     |   4.25   \n",
      "----------------------------------------------------------------------\n",
      "  18    |    -    |   0.240929   |   98.32   |  0.391689  |   87.64   |   5.73   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  19    |   12    |   0.238472   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "  19    |    -    |   0.238472   |   98.32   |  0.371730  |   89.20   |   5.75   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  20    |   12    |   0.237141   |     -      |     -     |   4.33   \n",
      "----------------------------------------------------------------------\n",
      "  20    |    -    |   0.237141   |   98.11   |  0.367394  |   90.77   |   5.81   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "{'val_accuracy': 90.76704545454545, 'f1': array([0.88235294, 0.92307692]), 'precision': array([1.        , 0.85714286]), 'recall': array([0.78947368, 1.        ])}\n",
      "90.76704545454545\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0\n",
      " 1 1 1 0 1 0]\n",
      "(43,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   12    |   0.697804   |     -      |     -     |   4.27   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.697804   |   57.92   |  0.660376  |   62.93   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   12    |   0.655793   |     -      |     -     |   4.25   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.655793   |   77.25   |  0.591413  |   82.81   |   5.73   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   12    |   0.579466   |     -      |     -     |   4.30   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.579466   |   92.12   |  0.512097  |   90.62   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   12    |   0.503342   |     -      |     -     |   4.33   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.503342   |   96.63   |  0.476007  |   90.62   |   5.81   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   12    |   0.442607   |     -      |     -     |   4.31   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.442607   |   97.84   |  0.439334  |   92.19   |   5.79   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |   12    |   0.387160   |     -      |     -     |   4.31   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.387160   |   98.08   |  0.404931  |   92.19   |   5.79   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   7    |   12    |   0.352765   |     -      |     -     |   4.32   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.352765   |   98.32   |  0.391839  |   92.19   |   5.80   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |   12    |   0.328760   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.328760   |   98.56   |  0.345843  |   95.31   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   9    |   12    |   0.310359   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.310359   |   98.56   |  0.338676  |   93.75   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  10    |   12    |   0.293357   |     -      |     -     |   4.31   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.293357   |   98.56   |  0.356686  |   92.19   |   5.79   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  11    |   12    |   0.285730   |     -      |     -     |   4.32   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.285730   |   98.08   |  0.306711  |   95.31   |   5.81   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  12    |   12    |   0.272646   |     -      |     -     |   4.35   \n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   0.272646   |   98.80   |  0.315309  |   93.75   |   5.82   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  13    |   12    |   0.262005   |     -      |     -     |   4.36   \n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   0.262005   |   98.59   |  0.316352  |   93.75   |   5.84   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  14    |   12    |   0.256189   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "  14    |    -    |   0.256189   |   98.80   |  0.312667  |   93.75   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  15    |   12    |   0.247229   |     -      |     -     |   4.24   \n",
      "----------------------------------------------------------------------\n",
      "  15    |    -    |   0.247229   |   98.80   |  0.305769  |   93.75   |   5.71   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  16    |   12    |   0.241861   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "  16    |    -    |   0.241861   |   98.80   |  0.303151  |   93.75   |   5.76   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  17    |   12    |   0.237638   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "  17    |    -    |   0.237638   |   98.80   |  0.302906  |   93.75   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  18    |   12    |   0.235452   |     -      |     -     |   4.29   \n",
      "----------------------------------------------------------------------\n",
      "  18    |    -    |   0.235452   |   98.80   |  0.301362  |   93.75   |   5.77   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  19    |   12    |   0.233414   |     -      |     -     |   4.26   \n",
      "----------------------------------------------------------------------\n",
      "  19    |    -    |   0.233414   |   98.59   |  0.300664  |   93.75   |   5.74   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Acc   |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  20    |   12    |   0.235827   |     -      |     -     |   4.30   \n",
      "----------------------------------------------------------------------\n",
      "  20    |    -    |   0.235827   |   98.80   |  0.300429  |   93.75   |   5.78   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "{'val_accuracy': 93.75, 'f1': array([0.88888889, 0.92      ]), 'precision': array([0.94117647, 0.88461538]), 'recall': array([0.84210526, 0.95833333])}\n",
      "93.75\n",
      "auc:  []\n",
      "accuracy:  [66.58653846153847, 65.14423076923077, 83.77403846153847, 71.39423076923077, 84.4951923076923, 69.71153846153847, 67.42788461538461, 78.4090909090909, 90.76704545454545, 93.75]\n",
      "recall:  [array([0.55, 0.68]), array([0.75, 0.6 ]), array([0.85, 0.88]), array([0.6 , 0.88]), array([0.7 , 0.96]), array([0.4 , 0.88]), array([0.55, 0.76]), array([0.73684211, 0.79166667]), array([0.78947368, 1.        ]), array([0.84210526, 0.95833333])]\n",
      "Mean-acc: 77.14597902097901\n",
      "Mean-f1: [0.71852911 0.7983548 ]\n",
      "Mean-recall: [0.67684211 0.839     ]\n",
      "Mean-Precision: [0.78146308 0.76762346]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "X = data.tweet.values\n",
    "y = data.label.values\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X, y)\n",
    "StratifiedKFold(n_splits=10, random_state=None, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "max_length = 160\n",
    "# kf = KFold(n_splits=5)\n",
    "# kf.get_n_splits(X)\n",
    "# print(kf)\n",
    "epochs=20\n",
    "learningrate=5e-5\n",
    "# KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "\n",
    "auc=[]\n",
    "acc=[]\n",
    "prec=[]\n",
    "recall=[]\n",
    "f1=[]\n",
    "i=1\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "#         print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_val,mdX_train,mdX_val = X[train_index], X[test_index],md_X[train_index],md_X[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "#         train_file='train'+str(i)+'.pkl'\n",
    "#         tlabel_file='tlabel'+str(i)+'.pkl'\n",
    "#         val_file='val'+str(i)+'.pkl'\n",
    "#         valabelfile='valabel'+str(i)+'.pkl'\n",
    "#         with open(train_file,'wb') as f:\n",
    "#              pickle.dump(X_train, f)\n",
    "#         with open(tlabel_file,'wb') as f:\n",
    "#              pickle.dump(y_train, f)\n",
    "#         with open(val_file,'wb') as f:\n",
    "#              pickle.dump(X_val, f)\n",
    "#         with open(valabelfile,'wb') as f:\n",
    "#              pickle.dump(y_val, f)\n",
    "        print(y_val)\n",
    "        print(y_val.shape)\n",
    "        train_inputs, train_masks = preprocessing_for_ctbert(X_train,max_length)\n",
    "        val_inputs, val_masks = preprocessing_for_ctbert(X_val,max_length)\n",
    "        # Convert other data types to torch.Tensor\n",
    "        train_labels = torch.tensor(y_train)\n",
    "        val_labels = torch.tensor(y_val)\n",
    "\n",
    "\n",
    "        md_train=torch.tensor(mdX_train).type(torch.FloatTensor)\n",
    "        md_val=torch.tensor(mdX_val).type(torch.FloatTensor)\n",
    "\n",
    "        # Create the DataLoader for our training set\n",
    "        train_data = TensorDataset(train_inputs, train_masks,md_train, train_labels)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "        # Create the DataLoader for our validation set\n",
    "        val_data = TensorDataset(val_inputs, val_masks,md_val, val_labels)\n",
    "        val_sampler = SequentialSampler(val_data)\n",
    "        val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "        \n",
    "        \n",
    "        ctbert_classifier, optimizer, scheduler = initialize_model(epochs,learningrate)\n",
    "        set_seed(42)    # Set seed for reproducibility\n",
    "        train(ctbert_classifier, train_dataloader, val_dataloader, epochs, evaluation=True)\n",
    "        # Compute predicted probabilities on the test set\n",
    "        probs = bert_predict(ctbert_classifier, val_dataloader)\n",
    "#         print(probs)\n",
    "#         print(probs.shape)\n",
    "#         evaluate_roc(probs, y_val)\n",
    "        val_loss,val_accuracy,preds= evaluate(ctbert_classifier, val_dataloader)\n",
    "        metrics=cval_eval(val_loss,val_accuracy,preds,y_val)\n",
    "        print(metrics['val_accuracy'])\n",
    "      # append model score\n",
    "        acc.append(metrics['val_accuracy'])\n",
    "        f1.append(metrics['f1'])\n",
    "        recall.append(metrics['recall'])\n",
    "        prec.append(metrics['precision'])\n",
    "        \n",
    "\n",
    "print(\"auc: \",auc)\n",
    "print(\"accuracy: \",acc)\n",
    "# print(\"f1: \",f1)\n",
    "print(\"recall: \",recall)\n",
    "# print(f\"Mean-acc: {sum(auc) / len(auc)}\")\n",
    "print(f\"Mean-acc: {sum(acc) / len(acc)}\")\n",
    "print(f\"Mean-f1: {sum(f1) / len(f1)}\")\n",
    "print(f\"Mean-recall: {sum(recall) / len(recall)}\")\n",
    "print(f\"Mean-Precision: {sum(prec) / len(prec)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-80b52ad64283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy,y_pred= evaluate(classifier, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_preds=[]\n",
    "for i in y_pred:\n",
    "    b=i.cpu().detach().numpy()\n",
    "    np_preds=np.append(np_preds,b,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_preds=np_preds.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.70      0.72      0.71        39\n",
      "        real       0.78      0.76      0.77        50\n",
      "\n",
      "   micro avg       0.74      0.74      0.74        89\n",
      "   macro avg       0.74      0.74      0.74        89\n",
      "weighted avg       0.74      0.74      0.74        89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_names = ['fake', 'real']\n",
    "print(classification_report(y_val, np_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "      hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "      hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "      hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "      plt.ylabel('True ')\n",
    "      plt.xlabel('Predicted');\n",
    "cm = confusion_matrix(y_val, np_preds)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7674\n",
      "Accuracy: 74.16%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(classifier, val_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 features without ratio works better than 5 features with TFF\n",
    "6 features with addition of creation time is better than 5 features AUC=84.4,\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        fake       0.82      0.70      0.76        20\n",
    "        real       0.79      0.88      0.83        25\n",
    "        \n",
    "        \n",
    "best perforimg features so far are TFF,fav_count, tweet_count and age of the created account.  AUC=94% and accuracy:84.4%\n",
    " precision    recall  f1-score   support\n",
    "\n",
    "        fake       0.88      0.75      0.81        20\n",
    "        real       0.82      0.92      0.87        25\n",
    "        micro avg       0.84      0.84      0.84        45\n",
    "        macro avg       0.85      0.83      0.84        45\n",
    "        weighted avg       0.85      0.84      0.84        45\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
